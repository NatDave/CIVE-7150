{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import importlib\n",
        "\n",
        "def install_if_not_installed(package):\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"{package} is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"{package} is not installed. Installing...\")\n",
        "        !pip install {package}\n",
        "        print(f\"{package} has been successfully installed.\")\n",
        "\n",
        "# Check and install ucimlrepo if not installed\n",
        "install_if_not_installed(\"ucimlrepo\")\n",
        "\n",
        "# Import and use ucimlrepo\n",
        "import ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3tSotl_YYSdh",
        "outputId": "8a7b8cd5-1033-402f-eb67-d2c4efdddeed"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ucimlrepo is already installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import other necessary modules\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from ucimlrepo import fetch_ucirepo\n",
        "\n",
        "import warnings     # filter warning messages\n",
        "warnings.simplefilter(action=\"ignore\")"
      ],
      "metadata": {
        "id": "5fy22E8AYzla"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define sigmoid, gradient descent and loss functions for LogReg\n",
        "\n",
        "def sigmoid(z):\n",
        "    \"\"\"\n",
        "    Sigmoid function to convert linear combination to probabilities.\n",
        "    \"\"\"\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def cross_entropy_loss(y_true, y_pred_prob):\n",
        "    \"\"\"\n",
        "    Compute the cross-entropy loss.\n",
        "    \"\"\"\n",
        "    return -np.mean(y_true * np.log(y_pred_prob) + (1 - y_true) * np.log(1 - y_pred_prob))\n",
        "\n",
        "def gradient_descent_logistic(X, y, alpha, num_iterations):\n",
        "    \"\"\"\n",
        "    Perform gradient descent to minimize the cross-entropy loss.\n",
        "    \"\"\"\n",
        "    num_samples, num_features = X.shape\n",
        "    theta = np.zeros(num_features + 1)  # Initialize model parameters to zero, including intercept\n",
        "\n",
        "    # Insert a column of ones at the beginning of X (intercept term)\n",
        "    X_with_intercept = np.hstack([np.ones((num_samples, 1)), X])\n",
        "\n",
        "    # fixed number of iterations\n",
        "    for _ in range(num_iterations):\n",
        "        # Calculate predictions\n",
        "        y_pred_prob = sigmoid(X_with_intercept @ theta)\n",
        "\n",
        "        # Calculate gradients\n",
        "        gradients = -(1 / num_samples) * X_with_intercept.T @ (y - y_pred_prob)\n",
        "\n",
        "        # Update parameters\n",
        "        theta -= alpha * gradients\n",
        "\n",
        "    return theta"
      ],
      "metadata": {
        "id": "QE3pysjDYnB6"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data into DataFrame\n",
        "spambase = fetch_ucirepo(id=94)\n",
        "X = pd.DataFrame(spambase.data.features)\n",
        "y = pd.DataFrame(spambase.data.targets)\n",
        "\n",
        "# Normalize the features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_normalized_values = scaler.fit_transform(X)\n",
        "X = pd.DataFrame(X_normalized_values, columns=X.columns)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
      ],
      "metadata": {
        "id": "u9QlyeveoPRL"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define learning rates and number of iterations\n",
        "learning_rates = [0.01, 0.1, 0.5]\n",
        "num_iterations_list = [10, 50, 100]\n",
        "\n",
        "# Initialize results dictionary\n",
        "results = {'Learning rate': [], 'Num_iterations': [],\n",
        "           'Accuracy': [], 'Precision': [],\n",
        "           'Recall': [], 'F1 score': []}\n",
        "\n",
        "# Iterate over learning rates and number of iterations\n",
        "for alpha in learning_rates:\n",
        "    for num_iterations in num_iterations_list:\n",
        "        # Perform gradient descent\n",
        "        theta = gradient_descent_logistic(X_train, y_train.values.ravel(),\n",
        "                                          alpha, num_iterations)\n",
        "\n",
        "        # Predict probabilities on the test set\n",
        "        y_test_pred_prob = sigmoid(np.hstack([np.ones((len(X_test), 1)), X_test.values]) @ theta)\n",
        "        y_test_pred = (y_test_pred_prob >= 0.5).astype(int)\n",
        "\n",
        "        # Calculate evaluation metrics for testing set\n",
        "        accuracy_test = accuracy_score(y_test, y_test_pred)\n",
        "        precision_test = precision_score(y_test, y_test_pred)\n",
        "        recall_test = recall_score(y_test, y_test_pred)\n",
        "        f1_test = f1_score(y_test, y_test_pred)\n",
        "\n",
        "        # Append results to the dictionary\n",
        "        results['Learning rate'].append(alpha)\n",
        "        results['Num_iterations'].append(num_iterations)\n",
        "        results['Accuracy'].append(accuracy_test)\n",
        "        results['Precision'].append(precision_test)\n",
        "        results['Recall'].append(recall_test)\n",
        "        results['F1 score'].append(f1_test)\n",
        "\n",
        "# Create a DataFrame from the results dictionary\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Display the results\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC4VLfoWZOuE",
        "outputId": "b3dbc18d-d0c0-4724-a26e-c02c3ac92c38"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning rate  Num_iterations  Accuracy  Precision    Recall  F1 score\n",
            "0           0.01              10  0.903562   0.890558  0.873684  0.882040\n",
            "1           0.01              50  0.905300   0.899563  0.867368  0.883173\n",
            "2           0.01             100  0.901825   0.898678  0.858947  0.878364\n",
            "3           0.10              10  0.901825   0.898678  0.858947  0.878364\n",
            "4           0.10              50  0.904431   0.919540  0.842105  0.879121\n",
            "5           0.10             100  0.908775   0.926267  0.846316  0.884488\n",
            "6           0.50              10  0.906169   0.919908  0.846316  0.881579\n",
            "7           0.50              50  0.912250   0.934884  0.846316  0.888398\n",
            "8           0.50             100  0.916594   0.939675  0.852632  0.894040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize results dictionary\n",
        "cross_entropy_results = {'Learning rate': [], 'Num_iterations': [],\n",
        "           'CE Loss 10': [], 'CE Loss 50': [], 'CE Loss 100': []}\n",
        "\n",
        "# Iterate over learning rates and number of iterations\n",
        "for alpha in learning_rates:\n",
        "    for num_iterations in num_iterations_list:\n",
        "        # Perform gradient descent\n",
        "        theta = gradient_descent_logistic(X_train, y_train.values.ravel(),\n",
        "                                          alpha, num_iterations)\n",
        "\n",
        "        # Calculate cross-entropy loss at different iterations\n",
        "        theta_10 = gradient_descent_logistic(X_train, y_train.values.ravel(), alpha, 10)\n",
        "        theta_50 = gradient_descent_logistic(X_train, y_train.values.ravel(), alpha, 50)\n",
        "        theta_100 = gradient_descent_logistic(X_train, y_train.values.ravel(), alpha, 100)\n",
        "\n",
        "        # Calculate cross-entropy loss at different iterations\n",
        "        y_train_pred_prob_10 = sigmoid(np.hstack([np.ones((len(X_train), 1)), X_train.values]) @ theta_10)\n",
        "        y_train_pred_prob_50 = sigmoid(np.hstack([np.ones((len(X_train), 1)), X_train.values]) @ theta_50)\n",
        "        y_train_pred_prob_100 = sigmoid(np.hstack([np.ones((len(X_train), 1)), X_train.values]) @ theta_100)\n",
        "\n",
        "        loss_10 = np.mean([cross_entropy_loss(y_train.iloc[i], y_train_pred_prob_10[i]) for i in range(len(X_train))])\n",
        "        loss_50 = np.mean([cross_entropy_loss(y_train.iloc[i], y_train_pred_prob_50[i]) for i in range(len(X_train))])\n",
        "        loss_100 = np.mean([cross_entropy_loss(y_train.iloc[i], y_train_pred_prob_100[i]) for i in range(len(X_train))])\n",
        "\n",
        "        # Append results to the dictionary\n",
        "        cross_entropy_results['Learning rate'].append(alpha)\n",
        "        cross_entropy_results['Num_iterations'].append(num_iterations)\n",
        "        cross_entropy_results['CE Loss 10'].append(loss_10)\n",
        "        cross_entropy_results['CE Loss 50'].append(loss_50)\n",
        "        cross_entropy_results['CE Loss 100'].append(loss_100)\n",
        "\n",
        "\n",
        "# Create a DataFrame from the results dictionary\n",
        "cross_entropy_results_df = pd.DataFrame(cross_entropy_results)\n",
        "\n",
        "# Display the results\n",
        "print(cross_entropy_results_df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzqjfxIJhNWu",
        "outputId": "ecdd8100-71d3-4501-bcb8-05d5f9b4f2b4"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Learning rate  Num_iterations  CE Loss 10  CE Loss 50  CE Loss 100\n",
            "0           0.01              10    0.651361    0.541717     0.468710\n",
            "1           0.01              50    0.651361    0.541717     0.468710\n",
            "2           0.01             100    0.651361    0.541717     0.468710\n",
            "3           0.10              10    0.465348    0.324753     0.290112\n",
            "4           0.10              50    0.465348    0.324753     0.290112\n",
            "5           0.10             100    0.465348    0.324753     0.290112\n",
            "6           0.50              10    0.320919    0.259935     0.244397\n",
            "7           0.50              50    0.320919    0.259935     0.244397\n",
            "8           0.50             100    0.320919    0.259935     0.244397\n"
          ]
        }
      ]
    }
  ]
}